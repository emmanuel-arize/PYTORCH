{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "data=load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the input features (shape of the independent variables)\n",
    "data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.target_names)\n",
    "print(data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data[:2]\n",
    "# converting the data into a dataframe\n",
    "train_data=pd.DataFrame(data.data,columns=data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   species\n",
       "0        0\n",
       "1        0\n",
       "2        0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting the target variable into a dataframe\n",
    "y_data=pd.DataFrame(data.target,columns=['species'])\n",
    "y_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatinating both the target and the input features into a single dataframe\n",
    "train_data[['species']]=y_data[['species']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "\n",
       "   species  \n",
       "0        0  \n",
       "1        0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming some columns\n",
    "train_data=train_data.rename(columns={\"sepal length (cm)\":\"sepal_length\",\"sepal width (cm)\":\"sepal_width\",\n",
    "                   \"petal length (cm)\":\"petal_length\",\"petal width (cm)\":\"petal_width\"})\n",
    "train_data.shape\n",
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  species\n",
       "0           5.1          3.5           1.4          0.2        0\n",
       "1           4.9          3.0           1.4          0.2        0\n",
       "2           4.7          3.2           1.3          0.2        0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -3\n",
       "1   -2\n",
       "2   -1\n",
       "3    0\n",
       "4    1\n",
       "5    2\n",
       "6    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a series which will be used to show how to select certain rows\n",
    "s = pd.Series(range(-3, 4))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    2\n",
       "6    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting rows greater than 1\n",
    "s[s>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A\n",
       "4  1\n",
       "5  2\n",
       "6  3"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=pd.DataFrame(s,columns=['A'])\n",
    "\n",
    "c=s[s['A']>0]\n",
    "c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A\n",
       "5  2\n",
       "6  3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning the data into a binary classification problem by dropping the virginica \n",
    "#(where does under species=2) rows from the data\n",
    "train_data=train_data[train_data['species'] <2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  species\n",
       "0           5.1          3.5           1.4          0.2        0\n",
       "1           4.9          3.0           1.4          0.2        0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the input data (100, 4)\n",
      "shape of the target variable (100, 1)\n"
     ]
    }
   ],
   "source": [
    "x_data=train_data[['sepal_length','sepal_width','petal_length','petal_width']]\n",
    "y_data=train_data[['species']]\n",
    "print('shape of the input data',x_data.shape)\n",
    "print('shape of the target variable',y_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data=np.array(x_data)\n",
    "y_data=np.array(y_data)\n",
    "x_data=torch.from_numpy(x_data)\n",
    "y_data=torch.from_numpy(y_data)\n",
    "print('the sheep for the x_data is',x_data.shape)\n",
    "print('the sheep for the y_data is',y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sheep for the x_data is torch.Size([100, 4])\n",
      "the sheep for the y_data is torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data=Variable(x_data)\n",
    "y_data=Variable(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid_with_multi_layers(nn.Module):\n",
    "    def __init__(self,input_dim):\n",
    "        super(Sigmoid_with_multi_layers,self).__init__()\n",
    "        # in this case in_features (is fixed)=the number of features from the data and \n",
    "        # out_features can be any arbitrary number\n",
    "        self.linear1=nn.Linear(input_dim,4)\n",
    "        # the numbers  can be any arbitrary number but in_feactures_of linear2=out_features_of_linear1\n",
    "        self.linear2=nn.Linear(4,3)\n",
    "        # the numbers  can be any arbitrary number but in_feactures_of linear3=out_features_of_linear2\n",
    "        # out_feature=y_variable \n",
    "        self.linear3=nn.Linear(3,1)\n",
    "    def forward(self,x):\n",
    "        l1=f.sigmoid(self.linear1(x))\n",
    "        l2=f.sigmoid(self.linear2(l1))\n",
    "        l3=f.sigmoid(self.linear3(l2))\n",
    "        return l3\n",
    "\n",
    "input_dim=x_data.shape[1]\n",
    "model=Sigmoid_with_multi_layers(input_dim)\n",
    "loss_function=nn.BCELoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.05,momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch \n",
      " 0 loss \n",
      " tensor(0.8365)\n",
      "epoch \n",
      " 1 loss \n",
      " tensor(0.8303)\n",
      "epoch \n",
      " 2 loss \n",
      " tensor(0.8214)\n",
      "epoch \n",
      " 3 loss \n",
      " tensor(0.8117)\n",
      "epoch \n",
      " 4 loss \n",
      " tensor(0.8019)\n",
      "epoch \n",
      " 5 loss \n",
      " tensor(0.7925)\n",
      "epoch \n",
      " 6 loss \n",
      " tensor(0.7838)\n",
      "epoch \n",
      " 7 loss \n",
      " tensor(0.7757)\n",
      "epoch \n",
      " 8 loss \n",
      " tensor(0.7684)\n",
      "epoch \n",
      " 9 loss \n",
      " tensor(0.7616)\n",
      "epoch \n",
      " 10 loss \n",
      " tensor(0.7555)\n",
      "epoch \n",
      " 11 loss \n",
      " tensor(0.7499)\n",
      "epoch \n",
      " 12 loss \n",
      " tensor(0.7448)\n",
      "epoch \n",
      " 13 loss \n",
      " tensor(0.7401)\n",
      "epoch \n",
      " 14 loss \n",
      " tensor(0.7359)\n",
      "epoch \n",
      " 15 loss \n",
      " tensor(0.7321)\n",
      "epoch \n",
      " 16 loss \n",
      " tensor(0.7287)\n",
      "epoch \n",
      " 17 loss \n",
      " tensor(0.7255)\n",
      "epoch \n",
      " 18 loss \n",
      " tensor(0.7227)\n",
      "epoch \n",
      " 19 loss \n",
      " tensor(0.7201)\n",
      "epoch \n",
      " 20 loss \n",
      " tensor(0.7178)\n",
      "epoch \n",
      " 21 loss \n",
      " tensor(0.7157)\n",
      "epoch \n",
      " 22 loss \n",
      " tensor(0.7137)\n",
      "epoch \n",
      " 23 loss \n",
      " tensor(0.7120)\n",
      "epoch \n",
      " 24 loss \n",
      " tensor(0.7104)\n",
      "epoch \n",
      " 25 loss \n",
      " tensor(0.7090)\n",
      "epoch \n",
      " 26 loss \n",
      " tensor(0.7077)\n",
      "epoch \n",
      " 27 loss \n",
      " tensor(0.7065)\n",
      "epoch \n",
      " 28 loss \n",
      " tensor(0.7055)\n",
      "epoch \n",
      " 29 loss \n",
      " tensor(0.7045)\n",
      "epoch \n",
      " 30 loss \n",
      " tensor(0.7036)\n",
      "epoch \n",
      " 31 loss \n",
      " tensor(0.7028)\n",
      "epoch \n",
      " 32 loss \n",
      " tensor(0.7021)\n",
      "epoch \n",
      " 33 loss \n",
      " tensor(0.7014)\n",
      "epoch \n",
      " 34 loss \n",
      " tensor(0.7009)\n",
      "epoch \n",
      " 35 loss \n",
      " tensor(0.7003)\n",
      "epoch \n",
      " 36 loss \n",
      " tensor(0.6998)\n",
      "epoch \n",
      " 37 loss \n",
      " tensor(0.6994)\n",
      "epoch \n",
      " 38 loss \n",
      " tensor(0.6990)\n",
      "epoch \n",
      " 39 loss \n",
      " tensor(0.6986)\n",
      "epoch \n",
      " 40 loss \n",
      " tensor(0.6983)\n",
      "epoch \n",
      " 41 loss \n",
      " tensor(0.6980)\n",
      "epoch \n",
      " 42 loss \n",
      " tensor(0.6977)\n",
      "epoch \n",
      " 43 loss \n",
      " tensor(0.6974)\n",
      "epoch \n",
      " 44 loss \n",
      " tensor(0.6972)\n",
      "epoch \n",
      " 45 loss \n",
      " tensor(0.6970)\n",
      "epoch \n",
      " 46 loss \n",
      " tensor(0.6968)\n",
      "epoch \n",
      " 47 loss \n",
      " tensor(0.6966)\n",
      "epoch \n",
      " 48 loss \n",
      " tensor(0.6965)\n",
      "epoch \n",
      " 49 loss \n",
      " tensor(0.6963)\n",
      "epoch \n",
      " 50 loss \n",
      " tensor(0.6962)\n",
      "epoch \n",
      " 51 loss \n",
      " tensor(0.6961)\n",
      "epoch \n",
      " 52 loss \n",
      " tensor(0.6960)\n",
      "epoch \n",
      " 53 loss \n",
      " tensor(0.6959)\n",
      "epoch \n",
      " 54 loss \n",
      " tensor(0.6958)\n",
      "epoch \n",
      " 55 loss \n",
      " tensor(0.6957)\n",
      "epoch \n",
      " 56 loss \n",
      " tensor(0.6956)\n",
      "epoch \n",
      " 57 loss \n",
      " tensor(0.6955)\n",
      "epoch \n",
      " 58 loss \n",
      " tensor(0.6955)\n",
      "epoch \n",
      " 59 loss \n",
      " tensor(0.6954)\n",
      "epoch \n",
      " 60 loss \n",
      " tensor(0.6953)\n",
      "epoch \n",
      " 61 loss \n",
      " tensor(0.6953)\n",
      "epoch \n",
      " 62 loss \n",
      " tensor(0.6952)\n",
      "epoch \n",
      " 63 loss \n",
      " tensor(0.6952)\n",
      "epoch \n",
      " 64 loss \n",
      " tensor(0.6951)\n",
      "epoch \n",
      " 65 loss \n",
      " tensor(0.6951)\n",
      "epoch \n",
      " 66 loss \n",
      " tensor(0.6951)\n",
      "epoch \n",
      " 67 loss \n",
      " tensor(0.6950)\n",
      "epoch \n",
      " 68 loss \n",
      " tensor(0.6950)\n",
      "epoch \n",
      " 69 loss \n",
      " tensor(0.6950)\n",
      "epoch \n",
      " 70 loss \n",
      " tensor(0.6949)\n",
      "epoch \n",
      " 71 loss \n",
      " tensor(0.6949)\n",
      "epoch \n",
      " 72 loss \n",
      " tensor(0.6949)\n",
      "epoch \n",
      " 73 loss \n",
      " tensor(0.6948)\n",
      "epoch \n",
      " 74 loss \n",
      " tensor(0.6948)\n",
      "epoch \n",
      " 75 loss \n",
      " tensor(0.6948)\n",
      "epoch \n",
      " 76 loss \n",
      " tensor(0.6948)\n",
      "epoch \n",
      " 77 loss \n",
      " tensor(0.6947)\n",
      "epoch \n",
      " 78 loss \n",
      " tensor(0.6947)\n",
      "epoch \n",
      " 79 loss \n",
      " tensor(0.6947)\n",
      "epoch \n",
      " 80 loss \n",
      " tensor(0.6947)\n",
      "epoch \n",
      " 81 loss \n",
      " tensor(0.6947)\n",
      "epoch \n",
      " 82 loss \n",
      " tensor(0.6946)\n",
      "epoch \n",
      " 83 loss \n",
      " tensor(0.6946)\n",
      "epoch \n",
      " 84 loss \n",
      " tensor(0.6946)\n",
      "epoch \n",
      " 85 loss \n",
      " tensor(0.6946)\n",
      "epoch \n",
      " 86 loss \n",
      " tensor(0.6946)\n",
      "epoch \n",
      " 87 loss \n",
      " tensor(0.6946)\n",
      "epoch \n",
      " 88 loss \n",
      " tensor(0.6945)\n",
      "epoch \n",
      " 89 loss \n",
      " tensor(0.6945)\n",
      "epoch \n",
      " 90 loss \n",
      " tensor(0.6945)\n",
      "epoch \n",
      " 91 loss \n",
      " tensor(0.6945)\n",
      "epoch \n",
      " 92 loss \n",
      " tensor(0.6945)\n",
      "epoch \n",
      " 93 loss \n",
      " tensor(0.6945)\n",
      "epoch \n",
      " 94 loss \n",
      " tensor(0.6944)\n",
      "epoch \n",
      " 95 loss \n",
      " tensor(0.6944)\n",
      "epoch \n",
      " 96 loss \n",
      " tensor(0.6944)\n",
      "epoch \n",
      " 97 loss \n",
      " tensor(0.6944)\n",
      "epoch \n",
      " 98 loss \n",
      " tensor(0.6944)\n",
      "epoch \n",
      " 99 loss \n",
      " tensor(0.6944)\n",
      "epoch \n",
      " 100 loss \n",
      " tensor(0.6944)\n",
      "epoch \n",
      " 101 loss \n",
      " tensor(0.6943)\n",
      "epoch \n",
      " 102 loss \n",
      " tensor(0.6943)\n",
      "epoch \n",
      " 103 loss \n",
      " tensor(0.6943)\n",
      "epoch \n",
      " 104 loss \n",
      " tensor(0.6943)\n",
      "epoch \n",
      " 105 loss \n",
      " tensor(0.6943)\n",
      "epoch \n",
      " 106 loss \n",
      " tensor(0.6943)\n",
      "epoch \n",
      " 107 loss \n",
      " tensor(0.6943)\n",
      "epoch \n",
      " 108 loss \n",
      " tensor(0.6942)\n",
      "epoch \n",
      " 109 loss \n",
      " tensor(0.6942)\n",
      "epoch \n",
      " 110 loss \n",
      " tensor(0.6942)\n",
      "epoch \n",
      " 111 loss \n",
      " tensor(0.6942)\n",
      "epoch \n",
      " 112 loss \n",
      " tensor(0.6942)\n",
      "epoch \n",
      " 113 loss \n",
      " tensor(0.6942)\n",
      "epoch \n",
      " 114 loss \n",
      " tensor(0.6942)\n",
      "epoch \n",
      " 115 loss \n",
      " tensor(0.6941)\n",
      "epoch \n",
      " 116 loss \n",
      " tensor(0.6941)\n",
      "epoch \n",
      " 117 loss \n",
      " tensor(0.6941)\n",
      "epoch \n",
      " 118 loss \n",
      " tensor(0.6941)\n",
      "epoch \n",
      " 119 loss \n",
      " tensor(0.6941)\n",
      "epoch \n",
      " 120 loss \n",
      " tensor(0.6941)\n",
      "epoch \n",
      " 121 loss \n",
      " tensor(0.6941)\n",
      "epoch \n",
      " 122 loss \n",
      " tensor(0.6941)\n",
      "epoch \n",
      " 123 loss \n",
      " tensor(0.6940)\n",
      "epoch \n",
      " 124 loss \n",
      " tensor(0.6940)\n",
      "epoch \n",
      " 125 loss \n",
      " tensor(0.6940)\n",
      "epoch \n",
      " 126 loss \n",
      " tensor(0.6940)\n",
      "epoch \n",
      " 127 loss \n",
      " tensor(0.6940)\n",
      "epoch \n",
      " 128 loss \n",
      " tensor(0.6940)\n",
      "epoch \n",
      " 129 loss \n",
      " tensor(0.6940)\n",
      "epoch \n",
      " 130 loss \n",
      " tensor(0.6940)\n",
      "epoch \n",
      " 131 loss \n",
      " tensor(0.6939)\n",
      "epoch \n",
      " 132 loss \n",
      " tensor(0.6939)\n",
      "epoch \n",
      " 133 loss \n",
      " tensor(0.6939)\n",
      "epoch \n",
      " 134 loss \n",
      " tensor(0.6939)\n",
      "epoch \n",
      " 135 loss \n",
      " tensor(0.6939)\n",
      "epoch \n",
      " 136 loss \n",
      " tensor(0.6939)\n",
      "epoch \n",
      " 137 loss \n",
      " tensor(0.6939)\n",
      "epoch \n",
      " 138 loss \n",
      " tensor(0.6938)\n",
      "epoch \n",
      " 139 loss \n",
      " tensor(0.6938)\n",
      "epoch \n",
      " 140 loss \n",
      " tensor(0.6938)\n",
      "epoch \n",
      " 141 loss \n",
      " tensor(0.6938)\n",
      "epoch \n",
      " 142 loss \n",
      " tensor(0.6938)\n",
      "epoch \n",
      " 143 loss \n",
      " tensor(0.6938)\n",
      "epoch \n",
      " 144 loss \n",
      " tensor(0.6938)\n",
      "epoch \n",
      " 145 loss \n",
      " tensor(0.6938)\n",
      "epoch \n",
      " 146 loss \n",
      " tensor(0.6937)\n",
      "epoch \n",
      " 147 loss \n",
      " tensor(0.6937)\n",
      "epoch \n",
      " 148 loss \n",
      " tensor(0.6937)\n",
      "epoch \n",
      " 149 loss \n",
      " tensor(0.6937)\n",
      "epoch \n",
      " 150 loss \n",
      " tensor(0.6937)\n",
      "epoch \n",
      " 151 loss \n",
      " tensor(0.6937)\n",
      "epoch \n",
      " 152 loss \n",
      " tensor(0.6937)\n",
      "epoch \n",
      " 153 loss \n",
      " tensor(0.6937)\n",
      "epoch \n",
      " 154 loss \n",
      " tensor(0.6936)\n",
      "epoch \n",
      " 155 loss \n",
      " tensor(0.6936)\n",
      "epoch \n",
      " 156 loss \n",
      " tensor(0.6936)\n",
      "epoch \n",
      " 157 loss \n",
      " tensor(0.6936)\n",
      "epoch \n",
      " 158 loss \n",
      " tensor(0.6936)\n",
      "epoch \n",
      " 159 loss \n",
      " tensor(0.6936)\n",
      "epoch \n",
      " 160 loss \n",
      " tensor(0.6936)\n",
      "epoch \n",
      " 161 loss \n",
      " tensor(0.6935)\n",
      "epoch \n",
      " 162 loss \n",
      " tensor(0.6935)\n",
      "epoch \n",
      " 163 loss \n",
      " tensor(0.6935)\n",
      "epoch \n",
      " 164 loss \n",
      " tensor(0.6935)\n",
      "epoch \n",
      " 165 loss \n",
      " tensor(0.6935)\n",
      "epoch \n",
      " 166 loss \n",
      " tensor(0.6935)\n",
      "epoch \n",
      " 167 loss \n",
      " tensor(0.6935)\n",
      "epoch \n",
      " 168 loss \n",
      " tensor(0.6935)\n",
      "epoch \n",
      " 169 loss \n",
      " tensor(0.6934)\n",
      "epoch \n",
      " 170 loss \n",
      " tensor(0.6934)\n",
      "epoch \n",
      " 171 loss \n",
      " tensor(0.6934)\n",
      "epoch \n",
      " 172 loss \n",
      " tensor(0.6934)\n",
      "epoch \n",
      " 173 loss \n",
      " tensor(0.6934)\n",
      "epoch \n",
      " 174 loss \n",
      " tensor(0.6934)\n",
      "epoch \n",
      " 175 loss \n",
      " tensor(0.6934)\n",
      "epoch \n",
      " 176 loss \n",
      " tensor(0.6934)\n",
      "epoch \n",
      " 177 loss \n",
      " tensor(0.6933)\n",
      "epoch \n",
      " 178 loss \n",
      " tensor(0.6933)\n",
      "epoch \n",
      " 179 loss \n",
      " tensor(0.6933)\n",
      "epoch \n",
      " 180 loss \n",
      " tensor(0.6933)\n",
      "epoch \n",
      " 181 loss \n",
      " tensor(0.6933)\n",
      "epoch \n",
      " 182 loss \n",
      " tensor(0.6933)\n",
      "epoch \n",
      " 183 loss \n",
      " tensor(0.6933)\n",
      "epoch \n",
      " 184 loss \n",
      " tensor(0.6932)\n",
      "epoch \n",
      " 185 loss \n",
      " tensor(0.6932)\n",
      "epoch \n",
      " 186 loss \n",
      " tensor(0.6932)\n",
      "epoch \n",
      " 187 loss \n",
      " tensor(0.6932)\n",
      "epoch \n",
      " 188 loss \n",
      " tensor(0.6932)\n",
      "epoch \n",
      " 189 loss \n",
      " tensor(0.6932)\n",
      "epoch \n",
      " 190 loss \n",
      " tensor(0.6932)\n",
      "epoch \n",
      " 191 loss \n",
      " tensor(0.6932)\n",
      "epoch \n",
      " 192 loss \n",
      " tensor(0.6931)\n",
      "epoch \n",
      " 193 loss \n",
      " tensor(0.6931)\n",
      "epoch \n",
      " 194 loss \n",
      " tensor(0.6931)\n",
      "epoch \n",
      " 195 loss \n",
      " tensor(0.6931)\n",
      "epoch \n",
      " 196 loss \n",
      " tensor(0.6931)\n",
      "epoch \n",
      " 197 loss \n",
      " tensor(0.6931)\n",
      "epoch \n",
      " 198 loss \n",
      " tensor(0.6931)\n",
      "epoch \n",
      " 199 loss \n",
      " tensor(0.6931)\n",
      "epoch \n",
      " 200 loss \n",
      " tensor(0.6930)\n",
      "epoch \n",
      " 201 loss \n",
      " tensor(0.6930)\n",
      "epoch \n",
      " 202 loss \n",
      " tensor(0.6930)\n",
      "epoch \n",
      " 203 loss \n",
      " tensor(0.6930)\n",
      "epoch \n",
      " 204 loss \n",
      " tensor(0.6930)\n",
      "epoch \n",
      " 205 loss \n",
      " tensor(0.6930)\n",
      "epoch \n",
      " 206 loss \n",
      " tensor(0.6930)\n",
      "epoch \n",
      " 207 loss \n",
      " tensor(0.6929)\n",
      "epoch \n",
      " 208 loss \n",
      " tensor(0.6929)\n",
      "epoch \n",
      " 209 loss \n",
      " tensor(0.6929)\n",
      "epoch \n",
      " 210 loss \n",
      " tensor(0.6929)\n",
      "epoch \n",
      " 211 loss \n",
      " tensor(0.6929)\n",
      "epoch \n",
      " 212 loss \n",
      " tensor(0.6929)\n",
      "epoch \n",
      " 213 loss \n",
      " tensor(0.6929)\n",
      "epoch \n",
      " 214 loss \n",
      " tensor(0.6929)\n",
      "epoch \n",
      " 215 loss \n",
      " tensor(0.6928)\n",
      "epoch \n",
      " 216 loss \n",
      " tensor(0.6928)\n",
      "epoch \n",
      " 217 loss \n",
      " tensor(0.6928)\n",
      "epoch \n",
      " 218 loss \n",
      " tensor(0.6928)\n",
      "epoch \n",
      " 219 loss \n",
      " tensor(0.6928)\n",
      "epoch \n",
      " 220 loss \n",
      " tensor(0.6928)\n",
      "epoch \n",
      " 221 loss \n",
      " tensor(0.6928)\n",
      "epoch \n",
      " 222 loss \n",
      " tensor(0.6927)\n",
      "epoch \n",
      " 223 loss \n",
      " tensor(0.6927)\n",
      "epoch \n",
      " 224 loss \n",
      " tensor(0.6927)\n",
      "epoch \n",
      " 225 loss \n",
      " tensor(0.6927)\n",
      "epoch \n",
      " 226 loss \n",
      " tensor(0.6927)\n",
      "epoch \n",
      " 227 loss \n",
      " tensor(0.6927)\n",
      "epoch \n",
      " 228 loss \n",
      " tensor(0.6927)\n",
      "epoch \n",
      " 229 loss \n",
      " tensor(0.6926)\n",
      "epoch \n",
      " 230 loss \n",
      " tensor(0.6926)\n",
      "epoch \n",
      " 231 loss \n",
      " tensor(0.6926)\n",
      "epoch \n",
      " 232 loss \n",
      " tensor(0.6926)\n",
      "epoch \n",
      " 233 loss \n",
      " tensor(0.6926)\n",
      "epoch \n",
      " 234 loss \n",
      " tensor(0.6926)\n",
      "epoch \n",
      " 235 loss \n",
      " tensor(0.6926)\n",
      "epoch \n",
      " 236 loss \n",
      " tensor(0.6926)\n",
      "epoch \n",
      " 237 loss \n",
      " tensor(0.6925)\n",
      "epoch \n",
      " 238 loss \n",
      " tensor(0.6925)\n",
      "epoch \n",
      " 239 loss \n",
      " tensor(0.6925)\n",
      "epoch \n",
      " 240 loss \n",
      " tensor(0.6925)\n",
      "epoch \n",
      " 241 loss \n",
      " tensor(0.6925)\n",
      "epoch \n",
      " 242 loss \n",
      " tensor(0.6925)\n",
      "epoch \n",
      " 243 loss \n",
      " tensor(0.6925)\n",
      "epoch \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 244 loss \n",
      " tensor(0.6924)\n",
      "epoch \n",
      " 245 loss \n",
      " tensor(0.6924)\n",
      "epoch \n",
      " 246 loss \n",
      " tensor(0.6924)\n",
      "epoch \n",
      " 247 loss \n",
      " tensor(0.6924)\n",
      "epoch \n",
      " 248 loss \n",
      " tensor(0.6924)\n",
      "epoch \n",
      " 249 loss \n",
      " tensor(0.6924)\n",
      "epoch \n",
      " 250 loss \n",
      " tensor(0.6924)\n",
      "epoch \n",
      " 251 loss \n",
      " tensor(0.6923)\n",
      "epoch \n",
      " 252 loss \n",
      " tensor(0.6923)\n",
      "epoch \n",
      " 253 loss \n",
      " tensor(0.6923)\n",
      "epoch \n",
      " 254 loss \n",
      " tensor(0.6923)\n",
      "epoch \n",
      " 255 loss \n",
      " tensor(0.6923)\n",
      "epoch \n",
      " 256 loss \n",
      " tensor(0.6923)\n",
      "epoch \n",
      " 257 loss \n",
      " tensor(0.6923)\n",
      "epoch \n",
      " 258 loss \n",
      " tensor(0.6922)\n",
      "epoch \n",
      " 259 loss \n",
      " tensor(0.6922)\n",
      "epoch \n",
      " 260 loss \n",
      " tensor(0.6922)\n",
      "epoch \n",
      " 261 loss \n",
      " tensor(0.6922)\n",
      "epoch \n",
      " 262 loss \n",
      " tensor(0.6922)\n",
      "epoch \n",
      " 263 loss \n",
      " tensor(0.6922)\n",
      "epoch \n",
      " 264 loss \n",
      " tensor(0.6922)\n",
      "epoch \n",
      " 265 loss \n",
      " tensor(0.6921)\n",
      "epoch \n",
      " 266 loss \n",
      " tensor(0.6921)\n",
      "epoch \n",
      " 267 loss \n",
      " tensor(0.6921)\n",
      "epoch \n",
      " 268 loss \n",
      " tensor(0.6921)\n",
      "epoch \n",
      " 269 loss \n",
      " tensor(0.6921)\n",
      "epoch \n",
      " 270 loss \n",
      " tensor(0.6921)\n",
      "epoch \n",
      " 271 loss \n",
      " tensor(0.6920)\n",
      "epoch \n",
      " 272 loss \n",
      " tensor(0.6920)\n",
      "epoch \n",
      " 273 loss \n",
      " tensor(0.6920)\n",
      "epoch \n",
      " 274 loss \n",
      " tensor(0.6920)\n",
      "epoch \n",
      " 275 loss \n",
      " tensor(0.6920)\n",
      "epoch \n",
      " 276 loss \n",
      " tensor(0.6920)\n",
      "epoch \n",
      " 277 loss \n",
      " tensor(0.6920)\n",
      "epoch \n",
      " 278 loss \n",
      " tensor(0.6919)\n",
      "epoch \n",
      " 279 loss \n",
      " tensor(0.6919)\n",
      "epoch \n",
      " 280 loss \n",
      " tensor(0.6919)\n",
      "epoch \n",
      " 281 loss \n",
      " tensor(0.6919)\n",
      "epoch \n",
      " 282 loss \n",
      " tensor(0.6919)\n",
      "epoch \n",
      " 283 loss \n",
      " tensor(0.6919)\n",
      "epoch \n",
      " 284 loss \n",
      " tensor(0.6918)\n",
      "epoch \n",
      " 285 loss \n",
      " tensor(0.6918)\n",
      "epoch \n",
      " 286 loss \n",
      " tensor(0.6918)\n",
      "epoch \n",
      " 287 loss \n",
      " tensor(0.6918)\n",
      "epoch \n",
      " 288 loss \n",
      " tensor(0.6918)\n",
      "epoch \n",
      " 289 loss \n",
      " tensor(0.6918)\n",
      "epoch \n",
      " 290 loss \n",
      " tensor(0.6918)\n",
      "epoch \n",
      " 291 loss \n",
      " tensor(0.6917)\n",
      "epoch \n",
      " 292 loss \n",
      " tensor(0.6917)\n",
      "epoch \n",
      " 293 loss \n",
      " tensor(0.6917)\n",
      "epoch \n",
      " 294 loss \n",
      " tensor(0.6917)\n",
      "epoch \n",
      " 295 loss \n",
      " tensor(0.6917)\n",
      "epoch \n",
      " 296 loss \n",
      " tensor(0.6917)\n",
      "epoch \n",
      " 297 loss \n",
      " tensor(0.6916)\n",
      "epoch \n",
      " 298 loss \n",
      " tensor(0.6916)\n",
      "epoch \n",
      " 299 loss \n",
      " tensor(0.6916)\n",
      "epoch \n",
      " 300 loss \n",
      " tensor(0.6916)\n",
      "epoch \n",
      " 301 loss \n",
      " tensor(0.6916)\n",
      "epoch \n",
      " 302 loss \n",
      " tensor(0.6916)\n",
      "epoch \n",
      " 303 loss \n",
      " tensor(0.6915)\n",
      "epoch \n",
      " 304 loss \n",
      " tensor(0.6915)\n",
      "epoch \n",
      " 305 loss \n",
      " tensor(0.6915)\n",
      "epoch \n",
      " 306 loss \n",
      " tensor(0.6915)\n",
      "epoch \n",
      " 307 loss \n",
      " tensor(0.6915)\n",
      "epoch \n",
      " 308 loss \n",
      " tensor(0.6915)\n",
      "epoch \n",
      " 309 loss \n",
      " tensor(0.6914)\n",
      "epoch \n",
      " 310 loss \n",
      " tensor(0.6914)\n",
      "epoch \n",
      " 311 loss \n",
      " tensor(0.6914)\n",
      "epoch \n",
      " 312 loss \n",
      " tensor(0.6914)\n",
      "epoch \n",
      " 313 loss \n",
      " tensor(0.6914)\n",
      "epoch \n",
      " 314 loss \n",
      " tensor(0.6914)\n",
      "epoch \n",
      " 315 loss \n",
      " tensor(0.6913)\n",
      "epoch \n",
      " 316 loss \n",
      " tensor(0.6913)\n",
      "epoch \n",
      " 317 loss \n",
      " tensor(0.6913)\n",
      "epoch \n",
      " 318 loss \n",
      " tensor(0.6913)\n",
      "epoch \n",
      " 319 loss \n",
      " tensor(0.6913)\n",
      "epoch \n",
      " 320 loss \n",
      " tensor(0.6912)\n",
      "epoch \n",
      " 321 loss \n",
      " tensor(0.6912)\n",
      "epoch \n",
      " 322 loss \n",
      " tensor(0.6912)\n",
      "epoch \n",
      " 323 loss \n",
      " tensor(0.6912)\n",
      "epoch \n",
      " 324 loss \n",
      " tensor(0.6912)\n",
      "epoch \n",
      " 325 loss \n",
      " tensor(0.6912)\n",
      "epoch \n",
      " 326 loss \n",
      " tensor(0.6911)\n",
      "epoch \n",
      " 327 loss \n",
      " tensor(0.6911)\n",
      "epoch \n",
      " 328 loss \n",
      " tensor(0.6911)\n",
      "epoch \n",
      " 329 loss \n",
      " tensor(0.6911)\n",
      "epoch \n",
      " 330 loss \n",
      " tensor(0.6911)\n",
      "epoch \n",
      " 331 loss \n",
      " tensor(0.6910)\n",
      "epoch \n",
      " 332 loss \n",
      " tensor(0.6910)\n",
      "epoch \n",
      " 333 loss \n",
      " tensor(0.6910)\n",
      "epoch \n",
      " 334 loss \n",
      " tensor(0.6910)\n",
      "epoch \n",
      " 335 loss \n",
      " tensor(0.6910)\n",
      "epoch \n",
      " 336 loss \n",
      " tensor(0.6910)\n",
      "epoch \n",
      " 337 loss \n",
      " tensor(0.6909)\n",
      "epoch \n",
      " 338 loss \n",
      " tensor(0.6909)\n",
      "epoch \n",
      " 339 loss \n",
      " tensor(0.6909)\n",
      "epoch \n",
      " 340 loss \n",
      " tensor(0.6909)\n",
      "epoch \n",
      " 341 loss \n",
      " tensor(0.6909)\n",
      "epoch \n",
      " 342 loss \n",
      " tensor(0.6908)\n",
      "epoch \n",
      " 343 loss \n",
      " tensor(0.6908)\n",
      "epoch \n",
      " 344 loss \n",
      " tensor(0.6908)\n",
      "epoch \n",
      " 345 loss \n",
      " tensor(0.6908)\n",
      "epoch \n",
      " 346 loss \n",
      " tensor(0.6908)\n",
      "epoch \n",
      " 347 loss \n",
      " tensor(0.6907)\n",
      "epoch \n",
      " 348 loss \n",
      " tensor(0.6907)\n",
      "epoch \n",
      " 349 loss \n",
      " tensor(0.6907)\n",
      "epoch \n",
      " 350 loss \n",
      " tensor(0.6907)\n",
      "epoch \n",
      " 351 loss \n",
      " tensor(0.6907)\n",
      "epoch \n",
      " 352 loss \n",
      " tensor(0.6906)\n",
      "epoch \n",
      " 353 loss \n",
      " tensor(0.6906)\n",
      "epoch \n",
      " 354 loss \n",
      " tensor(0.6906)\n",
      "epoch \n",
      " 355 loss \n",
      " tensor(0.6906)\n",
      "epoch \n",
      " 356 loss \n",
      " tensor(0.6906)\n",
      "epoch \n",
      " 357 loss \n",
      " tensor(0.6905)\n",
      "epoch \n",
      " 358 loss \n",
      " tensor(0.6905)\n",
      "epoch \n",
      " 359 loss \n",
      " tensor(0.6905)\n",
      "epoch \n",
      " 360 loss \n",
      " tensor(0.6905)\n",
      "epoch \n",
      " 361 loss \n",
      " tensor(0.6905)\n",
      "epoch \n",
      " 362 loss \n",
      " tensor(0.6904)\n",
      "epoch \n",
      " 363 loss \n",
      " tensor(0.6904)\n",
      "epoch \n",
      " 364 loss \n",
      " tensor(0.6904)\n",
      "epoch \n",
      " 365 loss \n",
      " tensor(0.6904)\n",
      "epoch \n",
      " 366 loss \n",
      " tensor(0.6903)\n",
      "epoch \n",
      " 367 loss \n",
      " tensor(0.6903)\n",
      "epoch \n",
      " 368 loss \n",
      " tensor(0.6903)\n",
      "epoch \n",
      " 369 loss \n",
      " tensor(0.6903)\n",
      "epoch \n",
      " 370 loss \n",
      " tensor(0.6903)\n",
      "epoch \n",
      " 371 loss \n",
      " tensor(0.6902)\n",
      "epoch \n",
      " 372 loss \n",
      " tensor(0.6902)\n",
      "epoch \n",
      " 373 loss \n",
      " tensor(0.6902)\n",
      "epoch \n",
      " 374 loss \n",
      " tensor(0.6902)\n",
      "epoch \n",
      " 375 loss \n",
      " tensor(0.6901)\n",
      "epoch \n",
      " 376 loss \n",
      " tensor(0.6901)\n",
      "epoch \n",
      " 377 loss \n",
      " tensor(0.6901)\n",
      "epoch \n",
      " 378 loss \n",
      " tensor(0.6901)\n",
      "epoch \n",
      " 379 loss \n",
      " tensor(0.6901)\n",
      "epoch \n",
      " 380 loss \n",
      " tensor(0.6900)\n",
      "epoch \n",
      " 381 loss \n",
      " tensor(0.6900)\n",
      "epoch \n",
      " 382 loss \n",
      " tensor(0.6900)\n",
      "epoch \n",
      " 383 loss \n",
      " tensor(0.6900)\n",
      "epoch \n",
      " 384 loss \n",
      " tensor(0.6899)\n",
      "epoch \n",
      " 385 loss \n",
      " tensor(0.6899)\n",
      "epoch \n",
      " 386 loss \n",
      " tensor(0.6899)\n",
      "epoch \n",
      " 387 loss \n",
      " tensor(0.6899)\n",
      "epoch \n",
      " 388 loss \n",
      " tensor(0.6898)\n",
      "epoch \n",
      " 389 loss \n",
      " tensor(0.6898)\n",
      "epoch \n",
      " 390 loss \n",
      " tensor(0.6898)\n",
      "epoch \n",
      " 391 loss \n",
      " tensor(0.6898)\n",
      "epoch \n",
      " 392 loss \n",
      " tensor(0.6897)\n",
      "epoch \n",
      " 393 loss \n",
      " tensor(0.6897)\n",
      "epoch \n",
      " 394 loss \n",
      " tensor(0.6897)\n",
      "epoch \n",
      " 395 loss \n",
      " tensor(0.6897)\n",
      "epoch \n",
      " 396 loss \n",
      " tensor(0.6896)\n",
      "epoch \n",
      " 397 loss \n",
      " tensor(0.6896)\n",
      "epoch \n",
      " 398 loss \n",
      " tensor(0.6896)\n",
      "epoch \n",
      " 399 loss \n",
      " tensor(0.6896)\n",
      "epoch \n",
      " 400 loss \n",
      " tensor(0.6895)\n",
      "epoch \n",
      " 401 loss \n",
      " tensor(0.6895)\n",
      "epoch \n",
      " 402 loss \n",
      " tensor(0.6895)\n",
      "epoch \n",
      " 403 loss \n",
      " tensor(0.6895)\n",
      "epoch \n",
      " 404 loss \n",
      " tensor(0.6894)\n",
      "epoch \n",
      " 405 loss \n",
      " tensor(0.6894)\n",
      "epoch \n",
      " 406 loss \n",
      " tensor(0.6894)\n",
      "epoch \n",
      " 407 loss \n",
      " tensor(0.6894)\n",
      "epoch \n",
      " 408 loss \n",
      " tensor(0.6893)\n",
      "epoch \n",
      " 409 loss \n",
      " tensor(0.6893)\n",
      "epoch \n",
      " 410 loss \n",
      " tensor(0.6893)\n",
      "epoch \n",
      " 411 loss \n",
      " tensor(0.6892)\n",
      "epoch \n",
      " 412 loss \n",
      " tensor(0.6892)\n",
      "epoch \n",
      " 413 loss \n",
      " tensor(0.6892)\n",
      "epoch \n",
      " 414 loss \n",
      " tensor(0.6892)\n",
      "epoch \n",
      " 415 loss \n",
      " tensor(0.6891)\n",
      "epoch \n",
      " 416 loss \n",
      " tensor(0.6891)\n",
      "epoch \n",
      " 417 loss \n",
      " tensor(0.6891)\n",
      "epoch \n",
      " 418 loss \n",
      " tensor(0.6890)\n",
      "epoch \n",
      " 419 loss \n",
      " tensor(0.6890)\n",
      "epoch \n",
      " 420 loss \n",
      " tensor(0.6890)\n",
      "epoch \n",
      " 421 loss \n",
      " tensor(0.6890)\n",
      "epoch \n",
      " 422 loss \n",
      " tensor(0.6889)\n",
      "epoch \n",
      " 423 loss \n",
      " tensor(0.6889)\n",
      "epoch \n",
      " 424 loss \n",
      " tensor(0.6889)\n",
      "epoch \n",
      " 425 loss \n",
      " tensor(0.6888)\n",
      "epoch \n",
      " 426 loss \n",
      " tensor(0.6888)\n",
      "epoch \n",
      " 427 loss \n",
      " tensor(0.6888)\n",
      "epoch \n",
      " 428 loss \n",
      " tensor(0.6888)\n",
      "epoch \n",
      " 429 loss \n",
      " tensor(0.6887)\n",
      "epoch \n",
      " 430 loss \n",
      " tensor(0.6887)\n",
      "epoch \n",
      " 431 loss \n",
      " tensor(0.6887)\n",
      "epoch \n",
      " 432 loss \n",
      " tensor(0.6886)\n",
      "epoch \n",
      " 433 loss \n",
      " tensor(0.6886)\n",
      "epoch \n",
      " 434 loss \n",
      " tensor(0.6886)\n",
      "epoch \n",
      " 435 loss \n",
      " tensor(0.6885)\n",
      "epoch \n",
      " 436 loss \n",
      " tensor(0.6885)\n",
      "epoch \n",
      " 437 loss \n",
      " tensor(0.6885)\n",
      "epoch \n",
      " 438 loss \n",
      " tensor(0.6884)\n",
      "epoch \n",
      " 439 loss \n",
      " tensor(0.6884)\n",
      "epoch \n",
      " 440 loss \n",
      " tensor(0.6884)\n",
      "epoch \n",
      " 441 loss \n",
      " tensor(0.6883)\n",
      "epoch \n",
      " 442 loss \n",
      " tensor(0.6883)\n",
      "epoch \n",
      " 443 loss \n",
      " tensor(0.6883)\n",
      "epoch \n",
      " 444 loss \n",
      " tensor(0.6882)\n",
      "epoch \n",
      " 445 loss \n",
      " tensor(0.6882)\n",
      "epoch \n",
      " 446 loss \n",
      " tensor(0.6882)\n",
      "epoch \n",
      " 447 loss \n",
      " tensor(0.6881)\n",
      "epoch \n",
      " 448 loss \n",
      " tensor(0.6881)\n",
      "epoch \n",
      " 449 loss \n",
      " tensor(0.6881)\n",
      "epoch \n",
      " 450 loss \n",
      " tensor(0.6880)\n",
      "epoch \n",
      " 451 loss \n",
      " tensor(0.6880)\n",
      "epoch \n",
      " 452 loss \n",
      " tensor(0.6880)\n",
      "epoch \n",
      " 453 loss \n",
      " tensor(0.6879)\n",
      "epoch \n",
      " 454 loss \n",
      " tensor(0.6879)\n",
      "epoch \n",
      " 455 loss \n",
      " tensor(0.6879)\n",
      "epoch \n",
      " 456 loss \n",
      " tensor(0.6878)\n",
      "epoch \n",
      " 457 loss \n",
      " tensor(0.6878)\n",
      "epoch \n",
      " 458 loss \n",
      " tensor(0.6878)\n",
      "epoch \n",
      " 459 loss \n",
      " tensor(0.6877)\n",
      "epoch \n",
      " 460 loss \n",
      " tensor(0.6877)\n",
      "epoch \n",
      " 461 loss \n",
      " tensor(0.6876)\n",
      "epoch \n",
      " 462 loss \n",
      " tensor(0.6876)\n",
      "epoch \n",
      " 463 loss \n",
      " tensor(0.6876)\n",
      "epoch \n",
      " 464 loss \n",
      " tensor(0.6875)\n",
      "epoch \n",
      " 465 loss \n",
      " tensor(0.6875)\n",
      "epoch \n",
      " 466 loss \n",
      " tensor(0.6875)\n",
      "epoch \n",
      " 467 loss \n",
      " tensor(0.6874)\n",
      "epoch \n",
      " 468 loss \n",
      " tensor(0.6874)\n",
      "epoch \n",
      " 469 loss \n",
      " tensor(0.6873)\n",
      "epoch \n",
      " 470 loss \n",
      " tensor(0.6873)\n",
      "epoch \n",
      " 471 loss \n",
      " tensor(0.6873)\n",
      "epoch \n",
      " 472 loss \n",
      " tensor(0.6872)\n",
      "epoch \n",
      " 473 loss \n",
      " tensor(0.6872)\n",
      "epoch \n",
      " 474 loss \n",
      " tensor(0.6871)\n",
      "epoch \n",
      " 475 loss \n",
      " tensor(0.6871)\n",
      "epoch \n",
      " 476 loss \n",
      " tensor(0.6871)\n",
      "epoch \n",
      " 477 loss \n",
      " tensor(0.6870)\n",
      "epoch \n",
      " 478 loss \n",
      " tensor(0.6870)\n",
      "epoch \n",
      " 479 loss \n",
      " tensor(0.6869)\n",
      "epoch \n",
      " 480 loss \n",
      " tensor(0.6869)\n",
      "epoch \n",
      " 481 loss \n",
      " tensor(0.6869)\n",
      "epoch \n",
      " 482 loss \n",
      " tensor(0.6868)\n",
      "epoch \n",
      " 483 loss \n",
      " tensor(0.6868)\n",
      "epoch \n",
      " 484 loss \n",
      " tensor(0.6867)\n",
      "epoch \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 485 loss \n",
      " tensor(0.6867)\n",
      "epoch \n",
      " 486 loss \n",
      " tensor(0.6866)\n",
      "epoch \n",
      " 487 loss \n",
      " tensor(0.6866)\n",
      "epoch \n",
      " 488 loss \n",
      " tensor(0.6865)\n",
      "epoch \n",
      " 489 loss \n",
      " tensor(0.6865)\n",
      "epoch \n",
      " 490 loss \n",
      " tensor(0.6865)\n",
      "epoch \n",
      " 491 loss \n",
      " tensor(0.6864)\n",
      "epoch \n",
      " 492 loss \n",
      " tensor(0.6864)\n",
      "epoch \n",
      " 493 loss \n",
      " tensor(0.6863)\n",
      "epoch \n",
      " 494 loss \n",
      " tensor(0.6863)\n",
      "epoch \n",
      " 495 loss \n",
      " tensor(0.6862)\n",
      "epoch \n",
      " 496 loss \n",
      " tensor(0.6862)\n",
      "epoch \n",
      " 497 loss \n",
      " tensor(0.6861)\n",
      "epoch \n",
      " 498 loss \n",
      " tensor(0.6861)\n",
      "epoch \n",
      " 499 loss \n",
      " tensor(0.6860)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for eporch in range(500):\n",
    "    # predicting y\n",
    "    y_pred=model(x_data.float())\n",
    "    # computing the loss\n",
    "    loss=loss_function(y_pred,y_data.float())\n",
    "    print('epoch \\n',eporch,'loss \\n',loss.data)\n",
    "    #print(' The True y values are',y_data,)\n",
    "    #print('and the predicted values for y by the model are')\n",
    "    \n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    \n",
    "    # zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    #perform backward-propagation\n",
    "    loss.backward()\n",
    "    # All optimizers implement a step() method, that updates the parameters. It can be used as:\n",
    "    #optimizer.step()\n",
    "    optimizer.step()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
