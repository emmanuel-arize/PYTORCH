{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Convolution-Operation\" data-toc-modified-id=\"Convolution-Operation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Convolution Operation</a></span></li><li><span><a href=\"#Discrete-Convolution\" data-toc-modified-id=\"Discrete-Convolution-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Discrete Convolution</a></span></li><li><span><a href=\"#CROSS-CORRELATION-OPERATIONS\" data-toc-modified-id=\"CROSS-CORRELATION-OPERATIONS-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>CROSS-CORRELATION OPERATIONS</a></span></li><li><span><a href=\"#Constraining-the-MLP\" data-toc-modified-id=\"Constraining-the-MLP-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Constraining the MLP</a></span></li><li><span><a href=\"#Translation-Invariance\" data-toc-modified-id=\"Translation-Invariance-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Translation Invariance</a></span></li><li><span><a href=\"#Convolutional-Layers\" data-toc-modified-id=\"Convolutional-Layers-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Convolutional Layers</a></span></li><li><span><a href=\"#Object-Edge-Detection-in-Images\" data-toc-modified-id=\"Object-Edge-Detection-in-Images-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Object Edge Detection in Images</a></span></li><li><span><a href=\"#Learning-a-Kernel\" data-toc-modified-id=\"Learning-a-Kernel-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Learning a Kernel</a></span></li><li><span><a href=\"#Padding\" data-toc-modified-id=\"Padding-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Padding</a></span></li><li><span><a href=\"#Stride\" data-toc-modified-id=\"Stride-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Stride</a></span></li><li><span><a href=\"#POOLING\" data-toc-modified-id=\"POOLING-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>POOLING</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Operation\n",
    "In its most general form convolution is a mathematical operator which takes two functions f and g and produces a third function s. It is the overlap of f and g as g is shifted over f and is defined as\n",
    "$$ s(t)=(f*g)(n)=\\int_{-\\infty}^{\\infty}f(m)g(n-m)dm $$\n",
    "\n",
    "In convolutional network terminology, the first argument f to the convolution is often referred to as the input and the second argument g as the kernel. The output is sometimes referred to as the feature map.\n",
    "\n",
    "Convolution is commutative  so it can be written as\n",
    "\n",
    "$$ s(t)=(f*g)(n)=\\int_{-\\infty}^{\\infty}f(n-m)g(m)dm $$\n",
    "## Discrete Convolution\n",
    "If we assume that f and g are defined only on integer n then we can define the discrete convolution as\n",
    "$$ s(t)=(f*g)(n)=\\sum_{m=-\\infty}^{\\infty}f(m)g(n-m) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use a two-dimensional image $I$ as our input, and probably a two-dimensional kernel (known as the convolution kernel, a filter, or simply the layer ºs weights that are often learnable parameters) $K$ of size $m \\times n$ . The image $I$ is convolved with the filter $K$ and produces the <b>feature map (channels) S</b>. The convolution operation is denoted by I*K and is mathematically given as\n",
    "$$ S(i,j)=(I \\times K)(i,j)=\\sum_{m}\\sum_{n}I(m,n)K(i-m,j-n)=\\sum_{m}\\sum_{n}I(i-m,j-n) K(m,n) $$ \n",
    "\n",
    "where $I(i,j)$  denote the pixel at location (i, j) in the input image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<cite style='color:blue'> The commutative property of convolution arises because we have flipped the kernel relative to the input, in the sense that as m increases, the index into the input increases, but the index into the kernel decreases. The only reason to flip the kernel is to obtain the commutative property. While the commutative property is useful for writing proofs, it is not usually an important property of a neural\n",
    "network implementation. <kbd>Instead, many neural network libraries implement a related function called the cross-correlation, which is the same as convolution but without flipping the kernel. Many machine learning libraries implement cross-correlation but call it convolution</kbd></cite>\n",
    "\n",
    "<p>(source: Deep Learning by Ian Goodfellow,Yoshua Bengio and Aaron Courville Chapter 9 (Convolutional Networks) page 332-333)</p>\n",
    "\n",
    "<cite style='color:blue'><kbd>strictly speaking, convolutional layers are a misnomer, since the operations they express are more accurately described as cross-correlations</kbd>. Based on our descriptions of convolutional layers, an input tensor and a kernel tensor are combined to produce an output tensor through a cross-correlation operation.</cite>\n",
    "\n",
    "  (source: Dive into Deep Learning by Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola page 233)\n",
    "  \n",
    "  \n",
    "-----\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The kernel $K$ is flipped relative to the input. If the kernel is not flipped, then convolution operation is the same as cross-correlation operation and defined as.\n",
    "\\begin{equation*}\n",
    "S(i,j)=(I \\times K)(i,j)=\\sum_{m}\\sum_{n}I(i+m,j+n)K(m,n)\n",
    "\\end{equation*}\n",
    "# NOTE\n",
    "<b>The input area on which a filter is applied is called local receptive field</b>\n",
    "<p><b>feature maps provide a spatialized set of learned features to the subsequent layer</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CROSS-CORRELATION OPERATIONS\n",
    "<img src=\"../images/cross1.jpg\" />\n",
    "<img src=\"../images/cross2.jpg\" />\n",
    "(source: Dive into Deep Learning by Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola page 233)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraining the MLP\n",
    "\n",
    "To start off, we can consider an MLP\n",
    "with two-dimensional images $\\mathbf{X}$ as inputs\n",
    "and their immediate hidden representations\n",
    "$\\mathbf{H}$ similarly represented as matrices in mathematics and as two-dimensional tensors in code, where both $\\mathbf{X}$ and $\\mathbf{H}$ have the same shape.\n",
    "Let that sink in.\n",
    "We now conceive of not only the inputs but\n",
    "also the hidden representations as possessing spatial structure.\n",
    "\n",
    "Let $[\\mathbf{X}]_{i, j}$ and $[\\mathbf{H}]_{i, j}$ denote the pixel\n",
    "at location ($i$, $j$)\n",
    "in the input image and hidden representation, respectively.\n",
    "Consequently, to have each of the hidden units\n",
    "receive input from each of the input pixels,\n",
    "we would switch from using weight matrices\n",
    "(as we did previously in MLPs)\n",
    "to representing our parameters\n",
    "as fourth-order weight tensors $\\mathsf{W}$.\n",
    "Suppose that $\\mathbf{U}$ contains biases,\n",
    "we could formally express the fully-connected layer as\n",
    "\n",
    "$$\\begin{aligned} \\left[\\mathbf{H}\\right]_{i, j} &= [\\mathbf{U}]_{i, j} + \\sum_k \\sum_l[\\mathsf{W}]_{i, j, k, l}  [\\mathbf{X}]_{k, l}\\\\ &=  [\\mathbf{U}]_{i, j} +\n",
    "\\sum_a \\sum_b [\\mathsf{V}]_{i, j, a, b}  [\\mathbf{X}]_{i+a, j+b}.\\end{aligned},$$\n",
    "\n",
    "where the switch from $\\mathsf{W}$ to $\\mathsf{V}$ is entirely cosmetic for now\n",
    "since there is a one-to-one correspondence\n",
    "between coefficients in both fourth-order tensors.\n",
    "We simply re-index the subscripts $(k, l)$\n",
    "such that $k = i+a$ and $l = j+b$.\n",
    "In other words, we set $[\\mathsf{V}]_{i, j, a, b} = [\\mathsf{W}]_{i, j, i+a, j+b}$.\n",
    "The indices $a$ and $b$ run over both positive and negative offsets,\n",
    "covering the entire image.\n",
    "For any given location ($i$, $j$) in the hidden representation $[\\mathbf{H}]_{i, j}$,\n",
    "we compute its value by summing over pixels in $x$,\n",
    "centered around $(i, j)$ and weighted by $[\\mathsf{V}]_{i, j, a, b}$.\n",
    "\n",
    "\n",
    "(source: Dive into Deep Learning by Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola page 229-230)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation Invariance\n",
    "The network should respond similarly to the same patch or object, regardless of where it appears in the image. <b>This principle is called translation invariance</b>.  For example, in the classification of the handwritten digits a particular patch or object should be assigned the same classification regardless of its position within the image .\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Now let us invoke the first principle\n",
    "established above: translation invariance.\n",
    "This implies that a shift in the input $\\mathbf{X}$\n",
    "should simply lead to a shift in the hidden representation $\\mathbf{H}$.\n",
    "This is only possible if $\\mathsf{V}$ and $\\mathbf{U}$ do not actually depend on $(i, j)$,\n",
    "i.e., we have $[\\mathsf{V}]_{i, j, a, b} = [\\mathbf{V}]_{a, b}$ and $\\mathbf{U}$ is a constant, say $u$.\n",
    "As a result, we can simplify the definition for $\\mathbf{H}$:\n",
    "\n",
    "$$[\\mathbf{H}]_{i, j} = u + \\sum_a\\sum_b [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}.$$\n",
    "\n",
    "\n",
    "This is a *convolution*!\n",
    "We are effectively weighting pixels at $(i+a, j+b)$\n",
    "in the vicinity of location $(i, j)$ with coefficients $[\\mathbf{V}]_{a, b}$\n",
    "to obtain the value $[\\mathbf{H}]_{i, j}$.\n",
    "Note that $[\\mathbf{V}]_{a, b}$ needs many fewer coefficients than $[\\mathsf{V}]_{i, j, a, b}$ since it\n",
    "no longer depends on the location within the image.\n",
    "We have made significant progress!\n",
    "\n",
    "(source: Dive into Deep Learning by Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola page 230)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X, K):\n",
    "    \"\"\"Compute 2D cross-correlation.\"\"\"\n",
    "    h,w=K.shape\n",
    "    Y=torch.zeros((X.shape[0]-h+1,X.shape[1]-w+1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i,j]=(X[i:i+h,j:j+w]*K).sum()\n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "I=np.array([[0,1,2],[3,4,5],[6,7,8]])\n",
    "K=np.array([[0,1],[2,3]])\n",
    "I=torch.from_numpy(I)\n",
    "K=torch.from_numpy(K)\n",
    "#K=torch.tensor([[0,1],[2,3]])\n",
    "#I=torch.tensor([[0,1,2],[3,4,5],[6,7,8]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(I,K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layers\n",
    "A convolutional layer cross-correlates the input and kernels and adds a scalar bias to produce an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.w=nn.Parameter(torch.rand(kernel_size))\n",
    "        self.b=nn.Parameter(torch.rand(1,))\n",
    "    def forward(self,inputs):\n",
    "        output=corr2d(inputs,self.w)+self.b\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Object Edge Detection in Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((6, 9))\n",
    "X[:, 2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, -1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = torch.tensor([[1, -1]])\n",
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter X and our designed kernel K to perform the cross-correlation operations. As you can see, we\n",
    "will detect 1 for the edge from white to black and -1 for the edge from black to white. The rest of\n",
    "the outputs are 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.,  0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X,K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us apply the kernel to the transposed image. As expected, it vanishes. The kernel K only detects\n",
    "vertical edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X.T,K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a Kernel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a convolutional layer with 1 output channel\n",
    "# (channels will be introduced in the following section)\n",
    "# and a kernel array shape of (1, 2)\n",
    "conv2d=nn.Conv2d(in_channels=1,out_channels=1,kernel_size=(1,2),bias=False)\n",
    "#conv2d=nn.Conv2d(in_channels=1,kernel_size=(1,2),out_channels=1)\n",
    "# or simply\n",
    "#conv2d=nn.Conv2d(1,1,kernel_size=(1,2),bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Conv2d(1, 1, kernel_size=(1, 2), stride=(1, 1), bias=False)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([[[[0.4192, 0.2840]]]]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = corr2d(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 8])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(1, 1, 6, 9)\n",
    "Y = Y.reshape(1, 1, 6, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2, loss 19.598\n",
      "batch 4, loss 17.485\n",
      "batch 6, loss 15.731\n",
      "batch 8, loss 14.269\n",
      "batch 10, loss 13.044\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    l.mean().backward()\n",
    "    # For the sake of simplicity, we ignore the bias here\n",
    "    conv2d.weight.data[:] -=  3e-2  * conv2d.weight.grad\n",
    "    #conv2d.bias.data[:] -= 3e-2 * conv2d.bias.grad\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print('batch %d, loss %.3f' % (i + 1, l.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3402, 0.0698]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data.reshape(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding \n",
    "<img src=\"../images/padding.jpg\" />\n",
    "(source: Dive into Deep Learning by Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola page 242)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stride\n",
    "<img src='../images/stride.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Input Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in(X, K):\n",
    "    # First, traverse along the 0th dimension (channel dimension) of X and K.\n",
    "    # Then, add them together by using * to turn the result list into a\n",
    "    # positional argument of the add_n function\n",
    "    return sum(corr2d(x,k) for x,k in zip(X,K))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(np.array([[[0, 1, 2], [3, 4, 5], [6, 7, 8]],[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]))\n",
    "K = torch.from_numpy(np.array([[[0, 1], [2, 3]], [[1, 2], [3, 4]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2],\n",
       "         [3, 4, 5],\n",
       "         [6, 7, 8]],\n",
       "\n",
       "        [[1, 2, 3],\n",
       "         [4, 5, 6],\n",
       "         [7, 8, 9]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in(X,K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTIPLE OUTPUT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out(X, K):\n",
    "    # Traverse along the 0th dimension of K, and each time, perform\n",
    "    # cross-correlation operations with input X. All of the results are merged\n",
    "    # together using the stack function\n",
    "    return np.stack([corr2d_multi_in(X, k) for k in K])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We construct a convolution kernel with 3 output channels by concatenating the kernel array K with K+1 (plus one for each element in K) and K+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=np.stack((K,K+1,K+2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 56.,  72.],\n",
       "        [104., 120.]],\n",
       "\n",
       "       [[ 76., 100.],\n",
       "        [148., 172.]],\n",
       "\n",
       "       [[ 96., 128.],\n",
       "        [192., 224.]]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POOLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2d(X, pool_size, mode='max'):\n",
    "    p_h,p_w=pool_size\n",
    "    Y=np.zeros((X.shape[0]-p_h+1,X.shape[1]-p_w+1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode=='max':\n",
    "                Y[i,j]=torch.max((X[i:i+p_h,j:j+p_w]))\n",
    "            if mode=='avg':\n",
    "                Y[i,j]=torch.mean(X[i:i+p_h,j:j+p_w])\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 5.],\n",
       "       [7., 8.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]],dtype=torch.float32)\n",
    "pool2d(X, (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 3.],\n",
       "       [5., 6.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d(X, (2, 2),mode='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "author": "mes",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
